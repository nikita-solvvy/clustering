{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding for org 26 ...\n",
      "Loading token queries for org 26 ...\n",
      "Sentence embedding ...\n",
      "Clustering ...\n",
      "Identifying sample queries ...\n",
      "done.\n",
      "Loading embedding for org 46 ...\n",
      "Loading token queries for org 46 ...\n",
      "Sentence embedding ...\n",
      "Clustering ...\n",
      "Identifying sample queries ...\n",
      "done.\n",
      "Loading embedding for org 54 ...\n",
      "Loading token queries for org 54 ...\n",
      "Sentence embedding ...\n",
      "Clustering ...\n",
      "Identifying sample queries ...\n",
      "done.\n",
      "Loading embedding for org 53 ...\n",
      "Loading token queries for org 53 ...\n",
      "Sentence embedding ...\n",
      "Clustering ...\n",
      "Identifying sample queries ...\n",
      "done.\n",
      "Loading embedding for org 42 ...\n",
      "Loading token queries for org 42 ...\n",
      "Sentence embedding ...\n",
      "Clustering ...\n",
      "Identifying sample queries ...\n",
      "done.\n",
      "Loading embedding for org 23 ...\n",
      "Loading token queries for org 23 ...\n",
      "Sentence embedding ...\n",
      "Clustering ...\n",
      "Identifying sample queries ...\n",
      "done.\n",
      "Loading embedding for org 21 ...\n",
      "Loading token queries for org 21 ...\n",
      "Sentence embedding ...\n",
      "Clustering ...\n",
      "Identifying sample queries ...\n",
      "done.\n",
      "Loading embedding for org 50 ...\n",
      "Loading token queries for org 50 ...\n",
      "Sentence embedding ...\n",
      "Clustering ...\n",
      "Identifying sample queries ...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import scipy\n",
    "import datetime\n",
    "\n",
    "#list_of_orgs = pd.read_csv('list_of_active_orgs', delimiter=' ', header=None).iloc[0]\n",
    "#org_id = 1\n",
    "\n",
    "list_of_orgs = [26,46,54,53,42,23,21,50]\n",
    "for org_id in list_of_orgs:\n",
    "    \n",
    "    print('Loading embedding for org',org_id,'...')\n",
    "    output_file = ''.join(['All_orgs_data/embedding',str(org_id),'.json'])\n",
    "    with open(output_file) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    embedding_dict = {item['word']:item['embedding'] for item in data}\n",
    "    embedding_len = len(list(embedding_dict.values())[0])\n",
    "\n",
    "    print('Loading token queries for org',org_id,'...')\n",
    "    output_file = ''.join(['All_orgs_data/joined_tokens_queries',str(org_id),'.json'])\n",
    "    with open(output_file) as data_file:\n",
    "        queries = json.load(data_file)    \n",
    "    \n",
    "    if len(queries) < 100:\n",
    "        continue\n",
    "    print('Sentence embedding ...') \n",
    "    X = pd.DataFrame(0, index=list(range(len(queries))), columns=list(range(embedding_len)) )\n",
    "    for i,query in enumerate(queries):\n",
    "        bag_of_words = query['content'].split()\n",
    "        sentence_embedding = np.zeros(embedding_len)\n",
    "        for item in bag_of_words:\n",
    "            try:\n",
    "                sentence_embedding = sentence_embedding+ embedding_dict[item]\n",
    "            except:\n",
    "                pass\n",
    "        X.loc[i] = sentence_embedding/(len(bag_of_words)+1) # to avoid Nan for 0 length queries\n",
    "\n",
    "    print('Clustering ...')    \n",
    "    \n",
    "    K = 10 # Number of cluster centers\n",
    "    N = 200 # Number of queries\n",
    "    kmeans = KMeans(n_clusters=K, random_state=0).fit(X)\n",
    "\n",
    "    cluster_roulette = [[] for i in range(K)]\n",
    "    for i,item in enumerate(kmeans.labels_):\n",
    "        cluster_roulette[item].append(i)\n",
    "\n",
    "    print('Identifying sample queries ...')  \n",
    "    i = 0\n",
    "    N_queries = []\n",
    "    while i < N and len(cluster_roulette)>0:\n",
    "        index = i%len(cluster_roulette)\n",
    "        if len(cluster_roulette[index]) ==0:\n",
    "            cluster_roulette.remove(index)\n",
    "        else:\n",
    "            \n",
    "            N_queries.append( queries[cluster_roulette[index].pop(0)]['content'].replace( '\\n', ''))\n",
    "            i+=1\n",
    "\n",
    "    file = open(''.join(['Query_output/','train_app_data_org-',str(org_id),'.csv']),'w') \n",
    "    for item in N_queries:\n",
    "        string = ''.join(['\\n-***********-\\n',item])\n",
    "        file.write(string)\n",
    "    file.close() \n",
    "    \n",
    "    STRING = ''.join(['Query_output/','train_app_data_org-',str(org_id),'.xlsx'])\n",
    "    df = pd.DataFrame({'Data': N_queries})\n",
    "    writer = pd.ExcelWriter(STRING, engine='xlsxwriter')\n",
    "    df.to_excel(writer,'Sheet1')\n",
    "    writer.save()\n",
    "\n",
    "    print('done.')\n",
    "    \n",
    "    #cluster_roulette\n",
    "\n",
    "    # # Looking at each cluster\n",
    "    # cluster_id = 3\n",
    "    # cluster = [queries[i]['content'] for i,item in enumerate(kmeans.labels_) if item==cluster_id]\n",
    "    # file = open(''.join(['Org-visualization',str(org_id),'/cluster',str(cluster_id)]),'w') \n",
    "    # for item in cluster:\n",
    "    #     file.write(\"%s\\n\" % item)\n",
    "    # file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Query_output/train_app_data_org-92'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file\n",
    "STRING = ''.join(['Query_output/','train_app_data_org-',str(org_id),'.xlsx'])\n",
    "#STRING = ''.join(['Query_output/','train_app_data_org-',str(org_id)])\n",
    "#df = pd.DataFrame({'col':N_queries})\n",
    "#df = pd.DataFrame({'Data': [10, 20, 30, 20, 15, 30, 45]})\n",
    "df = pd.DataFrame({'Data': N_queries})\n",
    "\n",
    "#df.to_csv(STRING)\n",
    "writer = pd.ExcelWriter(STRING, engine='xlsxwriter')\n",
    "\n",
    "df.to_excel(writer,'Sheet1')\n",
    "writer.save()\n",
    "\n",
    "#writer = pd.ExcelWriter('pandas_simple.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "#df.to_excel(writer, sheet_name='Sheet1')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "#writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "line = N_queries[1].replace( '\\n', '.')\n",
    "print(line)\n",
    "print(N_queries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "org_id=92\n",
    "output_file = ''.join(['All_orgs_data/joined_tokens_queries',str(org_id),'.json'])\n",
    "with open(output_file) as data_file:\n",
    "    queries = json.load(data_file)  \n",
    "print(queries[0],len(queries),X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#kmeans.cluster_centers_\n",
    "#kmeans.labels_\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(kmeans.labels_, bins='auto')  # arguments are passed to np.histogram\n",
    "\n",
    "#%wordcloud_cli.py --text ~/Desktop/queries --imagefile wordcloud.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans.labels_[229]\n",
    "N_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import scipy\n",
    "#X = pd.DataFrame(columns=tuple(range(embedding_len)))\n",
    "import datetime\n",
    "\n",
    "#list_of_orgs = pd.read_csv('list_of_active_orgs', delimiter=' ', header=None).iloc[0]\n",
    "#t1 = datetime.datetime.now()\n",
    "list_of_orgs = [1]\n",
    "\n",
    "for org_id in list_of_orgs: \n",
    "    # decode embedding as dictionary\n",
    "#     with open('embedding.json') as data_file:    \n",
    "#         data = json.load(data_file)\n",
    "\n",
    "    output_file = ''.join(['All_orgs_data/embedding',str(org_id),'.json'])\n",
    "    with open(output_file) as data_file:\n",
    "        data = json.load(data_file)\n",
    "\n",
    "    \n",
    "    embedding_dict = {item['word']:item['embedding'] for item in data}\n",
    "    embedding_len = len(embedding_dict)\n",
    "\n",
    "    # decode sentences from the json\n",
    "#     with open('joined_tokens_queries.json') as data_file:    \n",
    "#         queries = json.load(data_file)\n",
    "    \n",
    "    output_file = ''.join(['All_orgs_data/joined_tokens_queries',str(org_id),'.json'])\n",
    "    with open(output_file) as data_file:\n",
    "        queries = json.load(data_file)\n",
    "\n",
    "    X = pd.DataFrame(0, index=list(range(len(queries))), columns=list(range(embedding_len)) )\n",
    "    timee=[]\n",
    "    #t1 = datetime.datetime.now()\n",
    "    for i,query in enumerate(queries[:9]):\n",
    "\n",
    "        bag_of_words = query['content'].split()\n",
    "        if len(bag_of_words) == 0:\n",
    "            X.loc[i] = [0]*embedding_len\n",
    "        else:\n",
    "            sentence_embedding_matrix = pd.DataFrame(0, index=list(range(embedding_len)), columns=bag_of_words)\n",
    "            for item in bag_of_words:\n",
    "                try:\n",
    "                    sentence_embedding_matrix[item]=embedding_dict[item]\n",
    "                except:\n",
    "                    pass\n",
    "            X.loc[i] = pd.DataFrame(sentence_embedding_matrix).mean(axis=1)\n",
    "            #t2 = datetime.datetime.now()\n",
    "            # timee.append((t2-t1).microseconds/1000)\n",
    "\n",
    "            #X=X.drop(X.index[75418])\n",
    "            #queries[]\n",
    "            #X.isnull().values.any()\n",
    "            \n",
    "    kmeans = KMeans(n_clusters=100, random_state=0).fit(X)\n",
    "    #kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist = scipy.spatial.distance.cdist(X,kmeans.cluster_centers_) # pick the appropriate distance metric \n",
    "\n",
    "chosen_queries = []\n",
    "for item in dist.transpose():\n",
    "    idd = item.argmin(axis=0)\n",
    "    chosen_queries.append(idd)\n",
    "    dist[idd,:]= [float('inf')]*dist.shape[1]\n",
    "    \n",
    "if len(chosen_queries) == len(set(chosen_queries)):\n",
    "    print('unique')\n",
    "else:\n",
    "    print(len(set(chosen_queries)))\n",
    "QQ = [queries[item]['content'] for item in chosen_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import collections\n",
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import requests \n",
    "import datetime\n",
    "from heapq import heappush, heappushpop, nlargest\n",
    "import json\n",
    "MAX_QUERIES = 100000\n",
    "def download_and_process_all_queries( org_id):\n",
    "    queries_processed = 0\n",
    "    limit = 50\n",
    "    min_created_at_from_last_result = None\n",
    "    query_dump = []\n",
    "    while True:\n",
    "        params = {'org_id': org_id, 'limit': limit, 'fields': 'content,metadata.subject'}\n",
    "        if min_created_at_from_last_result is not None:\n",
    "            params['created_at$lt'] = min_created_at_from_last_result\n",
    "        \n",
    "        queries           = queries_api.get(params=params)\n",
    "        query_dump        = query_dump + queries\n",
    "        queries_processed = len(query_dump)\n",
    "        if queries_processed % 1000 == 0:\n",
    "            print('Fetched %d queries (total: %d)' % (len(queries), queries_processed))\n",
    "        if len(queries) == 0 or queries_processed >= MAX_QUERIES:\n",
    "            break\n",
    "        min_created_at_from_last_result = queries[-1]['created_at']\n",
    "\n",
    "    output_file = ''.join(['All_orgs_data/joined_tokens_queries',str(org_id),'.json'])\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(query_dump, outfile)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_of_orgs = pd.read_csv('list_of_active_orgs', delimiter=' ', header=None).iloc[0]\n",
    "t1 = datetime.datetime.now()\n",
    "\n",
    "for org_id in list_of_orgs:\n",
    "    print(org_id,'->ORG')\n",
    "    download_and_process_all_queries( org_id)\n",
    "    url         = ''.join(['http://localhost:3008/v1/artifacts/latest/data?org_id=',str(org_id),'&type=semantic_embedding'])\n",
    "    output_file = ''.join(['All_orgs_data/embedding',str(org_id),'.json'])\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(requests.get(url).json(), outfile)\n",
    "    \n",
    "t2 = datetime.datetime.now()\n",
    "timee.append((t2-t1).microseconds/1000)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
